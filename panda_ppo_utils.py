{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport io\nimport av\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom gymnasium.spaces import Box\n\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n\n\n\nclass ObservationWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        image_shape = (3, 224, 224)  \n        image_dim = np.prod(image_shape)  \n        coords_dim = 7  \n        \n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(image_dim + coords_dim,),\n            dtype=np.float32\n        )\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)), \n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n        ])\n\n    def observation(self, observation):\n        image = self.env.render()\n        image = Image.fromarray(image) \n        image = transform(image).numpy().flatten()  \n\n        coords = np.concatenate([\n            observation[\"observation\"][:3],  \n            observation[\"observation\"][6],\n            observation[\"observation\"][7:10],  \n        ])\n        \n        combined_obs = np.concatenate([image, coords])\n        expected_size = self.observation_space.shape[0]\n        if combined_obs.shape[0] != expected_size:\n            raise ValueError(f\"Problem: {combined_obs.shape[0]} != {expected_size}\")\n        \n        return combined_obs\n\n\nclass FeaturesExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim, feature_extractor):\n        super().__init__(observation_space, features_dim)\n        self.image_extractor = feature_extractor  \n        self.combined_fc = nn.Linear(512 + 7, features_dim)\n\n    def forward(self, observations):\n        batch = observations.shape[0]\n        \n        img_features = self.image_extractor(observations[:, :150528].reshape(batch, 3, 224, 224)).flatten(start_dim=1)\n        coord_features = observations[:, -7:]\n        \n        combined_features = torch.cat((img_features, coord_features), dim=1)\n        \n        return self.combined_fc(combined_features)\n\n\nclass ValidationCallback(BaseCallback):\n    def __init__(self, validation_env, validation_steps, output_filename=\"validation_video.mp4\", fps=10, max_steps=200, verbose=0):\n        super().__init__(verbose)\n        self.validation_env = validation_env\n        self.validation_steps = validation_steps\n        self.output_filename = output_filename\n        self.fps = fps\n        self.max_steps = max_steps\n        self.last_validation_step = 0\n\n    def _on_step(self):\n        if self.num_timesteps - self.last_validation_step >= self.validation_steps:\n            self.last_validation_step = self.num_timesteps\n            output_name = f\"{self.output_filename}_{self.num_timesteps}.mp4\"\n            if self.verbose > 0:\n                print(f\"Validation at {self.num_timesteps} steps.\")\n                \n            validate_video(\n                                    env=self.validation_env,\n                                    model=self.model,\n                                    output_filename=output_name,\n                                    fps=self.fps,\n                                    max_steps=self.max_steps\n            )\n            \n        return True\n\n\ndef make_video(numpy_images, output_filename=\"output_video.mp4\", fps=10):\n    container = av.open(output_filename, format='mp4', mode='w')\n    height, width, _ = numpy_images[0].shape  \n    stream = container.add_stream('h264', fps)\n    stream.height = height\n    stream.width = width\n    stream.pix_fmt = 'yuv420p'\n\n    for img in numpy_images:\n        if isinstance(img, torch.Tensor):\n            img = img.cpu().numpy()  \n            img = img.transpose(1, 2, 0)          \n            img = (img * 255).clip(0, 255).astype('uint8')  \n        \n        frame = av.VideoFrame.from_ndarray(img, format='rgb24')\n        for packet in stream.encode(frame):\n            container.mux(packet)\n\n    for packet in stream.encode(None):\n        container.mux(packet)\n    container.close()\n\n\ndef validate_video(env, model, output_filename=\"output_video.mp4\", fps=10, max_steps=200):\n    trajectory_images = []\n    obs, _ = env.reset()\n    done = False\n    step = 0\n\n    while not done and step < max_steps:\n        action, _ = model.predict(obs, deterministic=True)\n        obs, _, done, _, _ = env.step(action)\n        img = env.render() \n        if img.shape[0] == 3:  \n            img = img.transpose(1, 2, 0)  \n        trajectory_images.append(img)\n        step += 1\n\n    make_video(trajectory_images, output_filename, fps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:27:51.063031Z","iopub.execute_input":"2024-12-16T13:27:51.063440Z","iopub.status.idle":"2024-12-16T13:28:09.064875Z","shell.execute_reply.started":"2024-12-16T13:27:51.063400Z","shell.execute_reply":"2024-12-16T13:28:09.063933Z"}},"outputs":[],"execution_count":3}]}